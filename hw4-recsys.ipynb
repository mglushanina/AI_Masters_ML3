{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":359730,"sourceType":"datasetVersion","datasetId":156939},{"sourceId":6983391,"sourceType":"datasetVersion","datasetId":4013466}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n'''\nНужно реализовать (или использовать сделанный из предыдущих домашних работ)\n1. SVD-разложение\n2. Архитектуру Neural matrix factorization model (NeuMF) (см. лекцию и сем по нейронным сетям + https://arxiv.org/pdf/1708.05031.pdf +  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation), на вход которой подаются:\n• эмбединги длинны n по users \n• эмбединги длинны n по items  \n3. Гибридную архитектуру нейронную сеть, которая на вход получает:\n• эмбединги длинны n по users \n• эмбединги длинны n по items \n• признаки, которые можно извлечь из объекта (см.  baseline.ipynb, который есть в датасэт REKKO.)\n• признаки по пользователю (подумайте какие например можно извлечь из bookmarks.csv)\n\n\nДля гибридную архитектуру можно использовать NeuMF, рядом подать дополнительные входы. Их можно провязать полносвязанными слоями с основной частью. Можно это делать разными способами ближе к началу сети, или ближе концу. Попробуйте разные архитектуры, оцените их качество и скорость обучения. Обоснуйте свой финальный выбор.\nСравнить:\n• SVD-разложения (построенного на только rating.csv) на  8, 10, 12 компонент\n• Архитектуру NeuMF, где на вход подаются на SVD-разложения 8, 10, 12 компонент\n• Гибридную архитектуру, где на вход подаются на SVD-разложения 8, 10, 12 компонент\nДополнительно можно поисследовать какие-то параметры архитектур. \nДля оценки качества используйте кросс-валидацию на 3 фолда.\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nrekko_data = pd.read_csv('/kaggle/input/rekko-rating/ratings.csv')\nrekko_data.head()\nrekko_data.drop('ts', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T13:02:04.854955Z","iopub.execute_input":"2023-12-08T13:02:04.855789Z","iopub.status.idle":"2023-12-08T13:02:05.082475Z","shell.execute_reply.started":"2023-12-08T13:02:04.855754Z","shell.execute_reply":"2023-12-08T13:02:05.081655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_users = rekko_data.user_uid.unique()\nuser_to_index = {old: new for new, old in enumerate(unique_users)}\nnew_users = rekko_data.user_uid.map(user_to_index)\n\nunique_movies = rekko_data.element_uid.unique()\nmovie_to_index = {old: new for new, old in enumerate(unique_movies)}\nnew_movies = rekko_data.element_uid.map(movie_to_index)\n\nn_users = unique_users.shape[0]\nn_elements = unique_movies.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-08T12:59:23.069889Z","iopub.execute_input":"2023-12-08T12:59:23.070835Z","iopub.status.idle":"2023-12-08T12:59:23.693629Z","shell.execute_reply.started":"2023-12-08T12:59:23.070783Z","shell.execute_reply":"2023-12-08T12:59:23.692595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.DataFrame({'user_id': new_users, 'elem_id': new_movies})\ny = rekko_data['rating'].astype(np.float32)\n\n\nprint(f'Embeddings: {n_users} users, {n_elements} movies')\nprint(f'Dataset shape: {X.shape}')\nprint(f'Target shape: {y.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T12:59:25.018913Z","iopub.execute_input":"2023-12-08T12:59:25.019605Z","iopub.status.idle":"2023-12-08T12:59:25.028764Z","shell.execute_reply.started":"2023-12-08T12:59:25.019565Z","shell.execute_reply":"2023-12-08T12:59:25.027637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReviewsIterator:\n    \n    def __init__(self, X, y, batch_size=32, shuffle=True):\n        X, y = np.asarray(X), np.asarray(y)\n        \n        if shuffle:\n            index = np.random.permutation(X.shape[0])\n            X, y = X[index], y[index]\n            \n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n        self._current = 0\n        \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        return self.next()\n    \n    def next(self):\n        if self._current >= self.n_batches:\n            raise StopIteration()\n        k = self._current\n        self._current += 1\n        bs = self.batch_size\n        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]","metadata":{"execution":{"iopub.status.busy":"2023-12-08T12:59:25.326696Z","iopub.execute_input":"2023-12-08T12:59:25.327025Z","iopub.status.idle":"2023-12-08T12:59:25.336132Z","shell.execute_reply.started":"2023-12-08T12:59:25.326997Z","shell.execute_reply":"2023-12-08T12:59:25.335038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batches(X, y, bs=32, shuffle=True):\n    iterator = ReviewsIterator(X.index, y, bs, shuffle)  # Using X.index for DataFrame indices\n    for indices, (xb_indices, yb) in enumerate(iterator):\n        xb = X.loc[xb_indices].values  # Accessing rows by indices and converting to array\n        xb = torch.LongTensor(xb)\n        yb = torch.FloatTensor(yb)\n        yield xb, yb.view(-1, 1), xb_indices\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T12:59:26.179138Z","iopub.execute_input":"2023-12-08T12:59:26.180041Z","iopub.status.idle":"2023-12-08T12:59:26.185793Z","shell.execute_reply.started":"2023-12-08T12:59:26.179995Z","shell.execute_reply":"2023-12-08T12:59:26.184748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F \nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom itertools import zip_longest\n\n\nfor x_batch, y_batch, indices in batches(X, y, bs=4):\n    print(x_batch)\n    print(y_batch)\n    print(indices)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-12-08T12:59:26.415272Z","iopub.execute_input":"2023-12-08T12:59:26.415908Z","iopub.status.idle":"2023-12-08T12:59:29.354737Z","shell.execute_reply.started":"2023-12-08T12:59:26.415878Z","shell.execute_reply":"2023-12-08T12:59:29.353854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Смотрим на работу SVD**","metadata":{}},{"cell_type":"code","source":"#класс SVD для коллаборативной фильтрации на основе SGD \nimport numpy as np\nimport pandas as pd\n\nclass SVDWithSGD:\n    def __init__(self, num_factors=10, learning_rate=0.01, reg_param=0.02, num_epochs=20):\n        self.num_factors = num_factors\n        self.learning_rate = learning_rate\n        self.reg_param = reg_param\n        self.num_epochs = num_epochs\n\n    def fit(self, ratings_df):\n        self.user_col = 'user_uid'\n        self.item_col = 'element_uid'\n        self.rating_col = 'rating'\n\n        self.users = ratings_df[self.user_col].unique()\n        self.items = ratings_df[self.item_col].unique()\n\n        self.num_users = len(self.users)\n        self.num_items = len(self.items)\n\n        self.user_to_idx = {user: idx for idx, user in enumerate(self.users)}\n        self.item_to_idx = {item: idx for idx, item in enumerate(self.items)}\n\n        self.U = np.random.rand(self.num_users, self.num_factors)\n        self.V = np.random.rand(self.num_items, self.num_factors)\n\n        for epoch in range(self.num_epochs):\n            for _, row in ratings_df.iterrows():\n                user_idx = self.user_to_idx[row[self.user_col]]\n                item_idx = self.item_to_idx[row[self.item_col]]\n                rating = row[self.rating_col]\n\n                prediction = np.dot(self.U[user_idx], self.V[item_idx])\n                error = rating - prediction\n\n                # Update U and V using SGD\n                u_update = self.learning_rate * (error * self.V[item_idx] - self.reg_param * self.U[user_idx])\n                v_update = self.learning_rate * (error * self.U[user_idx] - self.reg_param * self.V[item_idx])\n\n                self.U[user_idx] += u_update\n                self.V[item_idx] += v_update\n            print('epoch', epoch, 'happened')\n\n    def predict(self, users, items):\n        predictions = []\n        for user, item in zip(users, items):\n            if user in self.user_to_idx and item in self.item_to_idx:\n                user_idx = self.user_to_idx[user]\n                item_idx = self.item_to_idx[item]\n                prediction = np.dot(self.U[user_idx], self.V[item_idx])\n\n                # Ensure the prediction is within the range of the target variable\n                # For example, rounding to the nearest integer for integer targets\n                prediction = int(round(prediction))  # Adjust this according to your target variable type\n\n                predictions.append(prediction)\n            else:\n                # Assign a default value or placeholder for missing users/items\n                # For example, using a minimum possible value for integer targets\n                predictions.append(0)  # Adjust this according to your target variable type\n        return predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:04:19.953613Z","iopub.execute_input":"2023-12-08T14:04:19.953964Z","iopub.status.idle":"2023-12-08T14:04:19.968455Z","shell.execute_reply.started":"2023-12-08T14:04:19.953937Z","shell.execute_reply":"2023-12-08T14:04:19.967477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#делаем разбивку на 5 фолдов для кроссвалидации \n#обучим модельку \n#протестируем \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_squared_error\n\n\nrandom_states = [40, 41, 42, 43, 44]\nfinal_score = []\n\nfor i in random_states: \n    X = rekko_data.drop('rating', axis=1)\n    y = rekko_data['rating']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i)\n    print('Random state:', i)\n    df_for_class = pd.concat([X_train, y_train], axis=1)\n    num_users = df_for_class['user_uid'].nunique()\n    num_items = df_for_class['element_uid'].nunique()\n\n    svd = SVDWithSGD(num_factors=10, learning_rate=0.01, reg_param=0.02, num_epochs=10)\n    svd.fit(df_for_class)\n    \n    preds = svd.predict(X_test['user_uid'], X_test['element_uid'])\n    final_score.append(mean_squared_error(y_test, preds, squared=False))\n    print('RMSE for SVD model is:', mean_squared_error(y_test, preds, squared=False))\n    \nprint('RMSE in cross-validation of 5 folds:', np.mean(final_score))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:07:15.053780Z","iopub.execute_input":"2023-12-08T14:07:15.054167Z","iopub.status.idle":"2023-12-08T14:29:13.680871Z","shell.execute_reply.started":"2023-12-08T14:07:15.054139Z","shell.execute_reply":"2023-12-08T14:29:13.679817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Теперь достанем новые данные из Rekko датасета помимо user-element пар**","metadata":{}},{"cell_type":"code","source":"#поработаем с bookmarks \n#что можно сделать - сделать бинарную переменную оставил ли пользователь фильм bookmark \n\nimport pandas as pd\nimport numpy as np\n\nbm_rekko_data = pd.read_csv('/kaggle/input/rekko-challenge/bookmarks.csv')\nbm_rekko_data.head()\nbm_rekko_data.drop('ts', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:51:57.660226Z","iopub.execute_input":"2023-12-08T14:51:57.661101Z","iopub.status.idle":"2023-12-08T14:51:58.290100Z","shell.execute_reply.started":"2023-12-08T14:51:57.661065Z","shell.execute_reply":"2023-12-08T14:51:58.289326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bookmarked_items = bm_rekko_data.groupby('user_uid')['element_uid'].apply(set).reset_index()\n\ndef is_bookmarked(row):\n    user_id = row['user_uid']\n    item_id = row['element_uid']\n    if user_id in bookmarked_items['user_uid'].values:\n        return int(item_id in bookmarked_items[bookmarked_items['user_uid'] == user_id]['element_uid'].iloc[0])\n    return 0\n\n#сделаем переменную добавлял ли пользователь элемент в избранное \nrekko_data['bookmarked'] = rekko_data.apply(is_bookmarked, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:51:58.416517Z","iopub.execute_input":"2023-12-08T14:51:58.416855Z","iopub.status.idle":"2023-12-08T14:54:42.236149Z","shell.execute_reply.started":"2023-12-08T14:51:58.416825Z","shell.execute_reply":"2023-12-08T14:54:42.235339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#подгружаем данные \n\nimport os\nimport json\n\nDATA_PATH = '/kaggle/input/rekko-challenge/'\n\nwith open(os.path.join(DATA_PATH, 'catalogue.json'), 'r') as f:\n    catalogue = json.load(f)\n    \ncatalogue = {int(k): v for k, v in catalogue.items()}","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:54:42.237685Z","iopub.execute_input":"2023-12-08T14:54:42.237988Z","iopub.status.idle":"2023-12-08T14:54:42.355794Z","shell.execute_reply.started":"2023-12-08T14:54:42.237960Z","shell.execute_reply":"2023-12-08T14:54:42.354994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#выписываем колонки для различных характеристик элементов \nfuture_cols = ['type', 'availability', 'duration', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'attributes']","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:54:42.356854Z","iopub.execute_input":"2023-12-08T14:54:42.357138Z","iopub.status.idle":"2023-12-08T14:54:42.361461Z","shell.execute_reply.started":"2023-12-08T14:54:42.357113Z","shell.execute_reply":"2023-12-08T14:54:42.360586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#делаем преобразования с данными \nelement_attributes_subset = {\n    element_id: {i: attributes[i] for i in future_cols} for element_id, attributes in catalogue.items()\n}\nrekko_data = rekko_data.merge(pd.DataFrame(element_attributes_subset).T, left_on='element_uid', right_index=True, how='left')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:54:42.363947Z","iopub.execute_input":"2023-12-08T14:54:42.364338Z","iopub.status.idle":"2023-12-08T14:54:42.962377Z","shell.execute_reply.started":"2023-12-08T14:54:42.364290Z","shell.execute_reply":"2023-12-08T14:54:42.961541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#преобразуем одну из переменных \nrekko_data['attributes_count'] = rekko_data['attributes'].apply(lambda x: len(x))\nrekko_data.drop('attributes', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:54:42.963560Z","iopub.execute_input":"2023-12-08T14:54:42.963846Z","iopub.status.idle":"2023-12-08T14:54:43.310190Z","shell.execute_reply.started":"2023-12-08T14:54:42.963821Z","shell.execute_reply":"2023-12-08T14:54:43.309363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#и ещё одну \nunique_values = set()\nfor sublist in rekko_data['availability']:\n    unique_values.update(sublist)\n\nfor value in unique_values:\n    rekko_data[value] = rekko_data['availability'].apply(lambda x: 1 if value in x else 0)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:55:17.007669Z","iopub.execute_input":"2023-12-08T14:55:17.008362Z","iopub.status.idle":"2023-12-08T14:55:18.038985Z","shell.execute_reply.started":"2023-12-08T14:55:17.008306Z","shell.execute_reply":"2023-12-08T14:55:18.038211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rekko_data.drop('availability', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:55:18.287521Z","iopub.execute_input":"2023-12-08T14:55:18.287910Z","iopub.status.idle":"2023-12-08T14:55:18.348691Z","shell.execute_reply.started":"2023-12-08T14:55:18.287882Z","shell.execute_reply":"2023-12-08T14:55:18.347938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#делаем класс для Neural Collaborative Filtering с использованием Matrix Factorization \n\nclass GMF(nn.Module):\n    def __init__(self, n_users, n_elements, n_factors):\n        super().__init__()\n        self.u_gmf = nn.Embedding(n_users, n_factors)\n        self.e_gmf = nn.Embedding(n_elements, n_factors)\n        self._init()\n\n    def _init(self):\n        torch.nn.init.xavier_uniform_(self.u_gmf.weight)\n        torch.nn.init.xavier_uniform_(self.e_gmf.weight)\n\n    def forward(self, users, movies):\n        u = self.u_gmf(users)\n        e = self.e_gmf(movies)\n        gmf_output = u * e  # Element-wise multiplication\n        return gmf_output\n    \n    \nclass MyNet(nn.Module): \n    def __init__(self, n_users, n_elements, n_factors, activation,\n                 hidden_units=10): \n        super().__init__()\n        hidden = self.get_list(hidden_units)\n        \n        def gen_layers(n_in):\n            \"\"\"\n            A generator that yields a sequence of hidden layers and \n            their activations/dropouts.\n            \n            Note that the function captures `hidden` and `dropouts` \n            values from the outer scope.\n            \"\"\"\n            nonlocal hidden\n            \n            for n_out in hidden:\n                yield nn.Linear(n_in, n_out)\n                yield activation()\n                n_in = n_out\n        \n        self.u = nn.Embedding(n_users, n_factors)\n        self.e = nn.Embedding(n_elements, n_factors)\n        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n        n_last = hidden[-1]\n        self.fc = nn.Linear(n_last, 1)\n        self._init\n        \n        \n    def get_list(self, n):\n        if isinstance(n, (int, float)):\n            return [n]\n        elif hasattr(n, '__iter__'):\n            return list(n)\n        \n    def _init(self):\n        \"\"\"\n        Setup embeddings and hidden layers with reasonable initial values.\n        \"\"\"\n        def init(m):\n            if type(m) == nn.Linear:\n                torch.nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.01)\n                \n        self.u.weight.data.uniform_(-0.05, 0.05)\n        self.m.weight.data.uniform_(-0.05, 0.05)\n        self.hidden.apply(init)\n        init(self.fc)\n        \n    def forward(self, users, movies, minmax=None):\n        features = torch.cat([self.u(users), self.e(movies)], dim=1)\n        x = self.hidden(features)\n        out = torch.sigmoid(self.fc(x))\n        if minmax is not None:\n            min_rating, max_rating = minmax\n            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n        return out\n        \n\n\nclass NCFWithGMF(nn.Module):\n    def __init__(self, n_users, n_elements, n_factors, activation, hidden_units=10):\n        super().__init__()\n        self.mlp = MyNet(n_users, n_elements, n_factors, activation, hidden_units)\n        self.gmf = GMF(n_users, n_elements, n_factors)\n        self.final_fc = nn.Linear(151, 1)\n        self._init()\n\n    def _init(self):\n        torch.nn.init.xavier_uniform_(self.final_fc.weight)\n        self.final_fc.bias.data.fill_(0.01)\n\n    def forward(self, users, movies, minmax=None):\n        mlp_output = self.mlp(users, movies)\n        gmf_output = self.gmf(users, movies)\n        combined_features = torch.cat([mlp_output, gmf_output], dim=1)\n        final_output = torch.sigmoid(self.final_fc(combined_features))\n        if minmax is not None:\n            min_rating, max_rating = minmax\n            final_output = final_output * (max_rating - min_rating + 1) + min_rating - 0.5\n        return final_output\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:55:45.571295Z","iopub.execute_input":"2023-12-08T14:55:45.572178Z","iopub.status.idle":"2023-12-08T14:55:45.591521Z","shell.execute_reply.started":"2023-12-08T14:55:45.572145Z","shell.execute_reply":"2023-12-08T14:55:45.590653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.DataFrame({'user_id': new_users, 'elem_id': new_movies})\ny = rekko_data['rating'].astype(np.float32)\n\n\nprint(f'Embeddings: {n_users} users, {n_elements} movies')\nprint(f'Dataset shape: {X.shape}')\nprint(f'Target shape: {y.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:58:07.275172Z","iopub.execute_input":"2023-12-08T14:58:07.275561Z","iopub.status.idle":"2023-12-08T14:58:07.285995Z","shell.execute_reply.started":"2023-12-08T14:58:07.275529Z","shell.execute_reply":"2023-12-08T14:58:07.285053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#настроим кросс валидацию \ndfs = {}\ndfs_sizes = {}\n\n# Define the number of folds\nnum_folds = 5\n\n# Calculate the size of each fold\nfold_size = len(X) // num_folds\n\nfor fold in range(num_folds):\n    start_index = fold * fold_size\n    end_index = (fold + 1) * fold_size if fold < num_folds - 1 else len(X)\n    \n    X_train = np.concatenate([X[:start_index], X[end_index:]], axis=0)\n    X_test = X[start_index:end_index]\n    y_train = np.concatenate([y[:start_index], y[end_index:]], axis=0)\n    y_test = y[start_index:end_index]\n    \n    # Create new dataframes for each fold\n    df_train = {'train': (X_train, y_train), 'val': (X_test, y_test)}\n    df_test_sizes = {'train': len(X_train), 'val': len(X_test)}\n    \n    dfs[f'fold_{fold + 1}'] = df_train\n    dfs_sizes[f'fold_{fold + 1}'] = df_test_sizes","metadata":{"execution":{"iopub.status.busy":"2023-12-08T15:02:23.175243Z","iopub.execute_input":"2023-12-08T15:02:23.176128Z","iopub.status.idle":"2023-12-08T15:02:23.191974Z","shell.execute_reply.started":"2023-12-08T15:02:23.176087Z","shell.execute_reply":"2023-12-08T15:02:23.191134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#здесь будет обучение \nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndf = {'train': (X_train, y_train), 'val': (X_test, y_test)}\ndf_sizes = {'train': len(X_train), 'val': len(X_test)}","metadata":{"execution":{"iopub.status.busy":"2023-12-08T14:59:35.355198Z","iopub.execute_input":"2023-12-08T14:59:35.355573Z","iopub.status.idle":"2023-12-08T14:59:35.390073Z","shell.execute_reply.started":"2023-12-08T14:59:35.355543Z","shell.execute_reply":"2023-12-08T14:59:35.389326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import mean_squared_error\n\ndef train_model(model, df, df_sizes, batches, epochs=10, lr=0.001, minmax=None):\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    train_data, train_labels = df['train']\n    val_data, val_labels = df['val']\n    \n    \n    batch_size = 2000\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        \n        # Training loop for collaborative filtering network\n        for xb, yb in batches(train_data, train_labels, bs=batch_size, shuffle=True):\n            users = xb[:, 0]  # Assuming user ID is at index 0\n            movies = xb[:, 1]  # Assuming movie ID is at index 1\n            ratings = yb.squeeze()  # Assuming ratings are single-dimensional\n\n            optimizer.zero_grad()\n            outputs = model(users, movies, minmax)\n            loss = criterion(outputs.squeeze(), ratings.float())\n            loss.backward()\n            optimizer.step()\n\n\n            running_loss += loss.item()\n        \n        # Calculating average loss per epoch\n        running_loss /= df_sizes['train']\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        predictions = []\n        true_ratings = []\n        with torch.no_grad():\n            for xb, yb in batches(val_data, val_labels, bs=batch_size, shuffle=False):\n                users = xb[:, 0]  # Assuming user ID is at index 0\n                movies = xb[:, 1]  # Assuming movie ID is at index 1\n                ratings = yb.squeeze()  # Assuming ratings are single-dimensional\n                \n                outputs = model(users, movies, minmax)\n                loss = criterion(outputs.squeeze(), ratings.float())\n                val_loss += loss.item()\n                predictions.extend(outputs.squeeze().tolist())\n                true_ratings.extend(ratings.tolist())\n\n            val_loss /= df_sizes['val']\n            val_rmse = mean_squared_error(true_ratings, predictions, squared=False)\n\n        print(f\"Epoch {epoch + 1}/{epochs}: \"\n              f\"Train Loss: {running_loss:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, \"\n              f\"Val RMSE: {val_rmse:.4f}\")\n\n    print('Training complete')\n    return val_rmse\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T15:10:05.621686Z","iopub.execute_input":"2023-12-08T15:10:05.622573Z","iopub.status.idle":"2023-12-08T15:10:05.635131Z","shell.execute_reply.started":"2023-12-08T15:10:05.622536Z","shell.execute_reply":"2023-12-08T15:10:05.634227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_range = rekko_data.rating.min(), rekko_data.rating.max()\nratings_range","metadata":{"execution":{"iopub.status.busy":"2023-12-08T15:10:07.893980Z","iopub.execute_input":"2023-12-08T15:10:07.894358Z","iopub.status.idle":"2023-12-08T15:10:07.901653Z","shell.execute_reply.started":"2023-12-08T15:10:07.894325Z","shell.execute_reply":"2023-12-08T15:10:07.900697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batches(X, y, bs=32, shuffle=True):\n    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n        xb = torch.LongTensor(xb)\n        yb = torch.FloatTensor(yb)\n        yield xb, yb.view(-1, 1) ","metadata":{"execution":{"iopub.status.busy":"2023-12-08T15:10:07.903376Z","iopub.execute_input":"2023-12-08T15:10:07.903683Z","iopub.status.idle":"2023-12-08T15:10:07.909305Z","shell.execute_reply.started":"2023-12-08T15:10:07.903657Z","shell.execute_reply":"2023-12-08T15:10:07.908513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_list = []\nfor i in dfs:  \n    data = pd.DataFrame()\n    model = NCFWithGMF(n_users, n_elements, n_factors=150, activation=nn.ReLU, hidden_units=[100, 100, 100])\n    res = train_model(model, dfs[i], dfs_sizes[i], batches, epochs=10, lr=0.001, minmax=ratings_range)\n    res_list.append(res)\n    \nprint('Mean RMSE on validation:', np.mean(res_list))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T15:10:08.835852Z","iopub.execute_input":"2023-12-08T15:10:08.836589Z","iopub.status.idle":"2023-12-08T15:47:22.791155Z","shell.execute_reply.started":"2023-12-08T15:10:08.836554Z","shell.execute_reply":"2023-12-08T15:47:22.790222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Теперь попробуем сделать сеть, которая сможет учитывать табличные данные**","metadata":{}},{"cell_type":"code","source":"class TabularNetwork(nn.Module):\n    def __init__(self, input_size, hidden_layers, output_size):\n        super().__init__()\n\n        layers = []\n        for i in range(len(hidden_layers)):\n            if i == 0:\n                layers.append(nn.Linear(input_size, hidden_layers[i]))\n            else:\n                layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n            layers.append(nn.ReLU())  # Using ReLU activation function\n\n        layers.append(nn.Linear(hidden_layers[-1], output_size))  # Output layer\n\n        self.tabular_layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.tabular_layers(x)\n\n\nclass HybridRecommendation(nn.Module):\n    def __init__(self, n_users, n_elements, n_factors, activation, input_size, output_size, hidden_layers, hidden_units=10):\n        super().__init__()\n        self.ncf_gmf = NCFWithGMF(n_users, n_elements, n_factors, activation, hidden_units)\n        self.tabular_network = TabularNetwork(input_size, hidden_layers, output_size)  # Replace with your tabular network\n\n        # Define a weight to combine predictions from both networks\n        self.weight = nn.Parameter(torch.tensor(0.5))  # Initial weight, can be learned\n\n    def forward(self, users, movies, tabular_data, minmax=None):\n        # Get predictions from NCFWithGMF\n        ncf_gmf_output = self.ncf_gmf(users, movies, minmax)\n\n        # Get predictions from the tabular network\n        tabular_output = self.tabular_network(tabular_data)\n\n        # Combine predictions using weighted average\n        weighted_output = self.weight * ncf_gmf_output + (1 - self.weight) * tabular_output\n\n        return weighted_output\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:42.979707Z","iopub.execute_input":"2023-12-08T16:18:42.980497Z","iopub.status.idle":"2023-12-08T16:18:42.991017Z","shell.execute_reply.started":"2023-12-08T16:18:42.980458Z","shell.execute_reply":"2023-12-08T16:18:42.990042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_tab_data = rekko_data.iloc[:, 1:]\nitem_rating_data = item_tab_data['rating']\nitem_tab_data.drop('rating', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:43.291071Z","iopub.execute_input":"2023-12-08T16:18:43.291419Z","iopub.status.idle":"2023-12-08T16:18:43.390850Z","shell.execute_reply.started":"2023-12-08T16:18:43.291390Z","shell.execute_reply":"2023-12-08T16:18:43.390049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_tab_data.drop('element_uid', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:44.828770Z","iopub.execute_input":"2023-12-08T16:18:44.829508Z","iopub.status.idle":"2023-12-08T16:18:44.876967Z","shell.execute_reply.started":"2023-12-08T16:18:44.829473Z","shell.execute_reply":"2023-12-08T16:18:44.876153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_encoded = pd.get_dummies(item_tab_data['type'])\n\n#сделаем нужный датасет \nitem_tab_data = pd.concat([item_tab_data, one_hot_encoded], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:45.695730Z","iopub.execute_input":"2023-12-08T16:18:45.696230Z","iopub.status.idle":"2023-12-08T16:18:45.800940Z","shell.execute_reply.started":"2023-12-08T16:18:45.696184Z","shell.execute_reply":"2023-12-08T16:18:45.799903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_tab_data.drop('type', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:58.144055Z","iopub.execute_input":"2023-12-08T16:18:58.144437Z","iopub.status.idle":"2023-12-08T16:18:58.184792Z","shell.execute_reply.started":"2023-12-08T16:18:58.144406Z","shell.execute_reply":"2023-12-08T16:18:58.183812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['duration', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5']","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:59.215361Z","iopub.execute_input":"2023-12-08T16:18:59.216003Z","iopub.status.idle":"2023-12-08T16:18:59.220328Z","shell.execute_reply.started":"2023-12-08T16:18:59.215968Z","shell.execute_reply":"2023-12-08T16:18:59.219367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_tab_data[cols] = item_tab_data[cols].astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:18:59.715768Z","iopub.execute_input":"2023-12-08T16:18:59.716087Z","iopub.status.idle":"2023-12-08T16:18:59.924050Z","shell.execute_reply.started":"2023-12-08T16:18:59.716059Z","shell.execute_reply":"2023-12-08T16:18:59.923055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_tab_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:19:01.948743Z","iopub.execute_input":"2023-12-08T16:19:01.949102Z","iopub.status.idle":"2023-12-08T16:19:01.973696Z","shell.execute_reply.started":"2023-12-08T16:19:01.949072Z","shell.execute_reply":"2023-12-08T16:19:01.972794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.DataFrame({'user_id': new_users, 'elem_id': new_movies})\ny = rekko_data['rating'].astype(np.float32)\n\n\nprint(f'Embeddings: {n_users} users, {n_elements} movies')\nprint(f'Dataset shape: {X.shape}')\nprint(f'Target shape: {y.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:19:02.319382Z","iopub.execute_input":"2023-12-08T16:19:02.319774Z","iopub.status.idle":"2023-12-08T16:19:02.329696Z","shell.execute_reply.started":"2023-12-08T16:19:02.319741Z","shell.execute_reply":"2023-12-08T16:19:02.328511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_hybrid_model(model, df, tabular_data_train, tabular_data_test, tabular_y_train, df_sizes, num_epochs=10, lr=0.001, batch_size=10000):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()  # Using Mean Squared Error loss for regression\n\n    train_data, train_labels = df['train']\n    val_data, val_labels = df['val']\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        # Training loop for collaborative filtering network\n        for xb, yb, indices in batches(train_data, train_labels, bs=batch_size, shuffle=True):\n            users = xb[:, 0]  # Assuming user ID is at index 0\n            movies = xb[:, 1]  # Assuming movie ID is at index 1\n            ratings = yb.squeeze()  # Assuming ratings are single-dimensional\n\n            optimizer.zero_grad()\n\n            # Extract corresponding tabular data for training\n            tabular_batch_train = pd.DataFrame([tabular_data_train.loc[i] for i in indices])\n            tabular_batch_train = torch.tensor(tabular_batch_train.values.astype(np.float64))\n            tabular_batch_train = tabular_batch_train.to(torch.float32)\n\n            # Forward pass for training\n            outputs = model(users, movies, tabular_batch_train, minmax=(1, 5))  # Assuming minmax range\n\n            # Calculate loss\n            loss = criterion(outputs.squeeze(), ratings)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        epoch_loss = running_loss / df_sizes['train']\n        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            users = torch.LongTensor(val_data['user_id'].values)\n            movies = torch.LongTensor(val_data['elem_id'].values)\n            ratings = torch.FloatTensor(val_labels.values)\n            td_test = torch.tensor(tabular_data_test.values.astype(np.float64))\n            td_test = td_test.to(torch.float32)\n\n            outputs = model(users, movies, td_test, minmax=(1, 5))  # Assuming minmax range\n\n            loss = criterion(outputs.squeeze(), ratings)\n            val_loss += loss.item()\n\n        val_loss /= df_sizes['val']\n        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n\n    print('Training finished.')\n    return val_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:28:59.703879Z","iopub.execute_input":"2023-12-08T16:28:59.704537Z","iopub.status.idle":"2023-12-08T16:28:59.717129Z","shell.execute_reply.started":"2023-12-08T16:28:59.704500Z","shell.execute_reply":"2023-12-08T16:28:59.716221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batches(X, y, bs=32, shuffle=True):\n    iterator = ReviewsIterator(X.index, y, bs, shuffle)  # Using X.index for DataFrame indices\n    for indices, (xb_indices, yb) in enumerate(iterator):\n        xb = X.loc[xb_indices].values  # Accessing rows by indices and converting to array\n        xb = torch.LongTensor(xb)\n        yb = torch.FloatTensor(yb)\n        yield xb, yb.view(-1, 1), xb_indices\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:26:11.287640Z","iopub.execute_input":"2023-12-08T16:26:11.288474Z","iopub.status.idle":"2023-12-08T16:26:11.294077Z","shell.execute_reply.started":"2023-12-08T16:26:11.288441Z","shell.execute_reply":"2023-12-08T16:26:11.293094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"default\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:26:11.939541Z","iopub.execute_input":"2023-12-08T16:26:11.940269Z","iopub.status.idle":"2023-12-08T16:26:11.944322Z","shell.execute_reply.started":"2023-12-08T16:26:11.940239Z","shell.execute_reply":"2023-12-08T16:26:11.943370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = [40, 41, 42, 43, 44]\nval_losses = []\n\nfor i in random_state: \n    print('Random state:', i)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    df = {'train': (X_train, y_train), 'val': (X_test, y_test)}\n    df_sizes = {'train': len(X_train), 'val': len(X_test)}\n    \n    tabular_data_train, tabular_data_test, tabular_y_train, tabular_y_test = train_test_split(item_tab_data, item_rating_data, test_size=0.2, random_state=i)\n    td_dim = tabular_data_train.shape[1]\n\n    model = HybridRecommendation(n_users, n_elements, n_factors=150, activation=nn.ReLU, hidden_units=[100], input_size=td_dim, output_size=1, hidden_layers=[16])\n    loss = train_hybrid_model(model, df, tabular_data_train, tabular_data_test, tabular_y_train, df_sizes, num_epochs=10, lr=0.001, batch_size=10000)\n    val_losses.append(loss)\n    \nprint('RMSE on validation in cross validation with 5 folds:', np.mean(val_losses))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T16:32:23.761886Z","iopub.execute_input":"2023-12-08T16:32:23.762733Z","iopub.status.idle":"2023-12-08T17:22:05.728057Z","shell.execute_reply.started":"2023-12-08T16:32:23.762694Z","shell.execute_reply":"2023-12-08T17:22:05.726812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Итоги: в моей реализации лучше всего сработал гибридный алгоритм, который учитывал много дополнительной информации, однако он оказывается несколько нестабильным. ","metadata":{}}]}